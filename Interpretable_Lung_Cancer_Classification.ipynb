{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdEmonMiaOfficial/Fusion-of-Clinical-Metadata-and-3D-CT-Image-Features-Interpretable-Lung-Cancer-Classification/blob/main/Interpretable_Lung_Cancer_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "5EOKa_nAKi-D",
        "outputId": "3b35c441-4631-42f7-f078-f047b2d90824"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-62a33485-c709-43f4-8f02-a4ce62cb18d0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-62a33485-c709-43f4-8f02-a4ce62cb18d0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preproc.zip to preproc.zip\n",
            "Total NPZ: 95\n",
            "Keys: ['x', 'y'] x shape: (1, 128, 128, 128) y: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "\n",
        "import os, zipfile\n",
        "ZIP_PATH = list(uploaded.keys())[0] if 'uploaded' in globals() else ZIP_PATH\n",
        "\n",
        "EXTRACT_DIR = \"/content/preproc\"\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "    zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "\n",
        "import os, numpy as np\n",
        "files_npz = [f for f in os.listdir(EXTRACT_DIR) if f.endswith(\".npz\")]\n",
        "print(\"Total NPZ:\", len(files_npz))\n",
        "d = np.load(os.path.join(EXTRACT_DIR, files_npz[0]))\n",
        "print(\"Keys:\", d.files, \"x shape:\", d[\"x\"].shape, \"y:\", d[\"y\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NPZ3DDataset(Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
        "        assert len(self.paths)>0, \"No .npz found!\"\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        d = np.load(self.paths[idx])\n",
        "        x = torch.from_numpy(d[\"x\"]).float()         # (1, D, H, W) in [0,1]\n",
        "        y = torch.tensor(int(d[\"y\"]), dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "ds = NPZ3DDataset(DATA_DIR)\n",
        "labels = [int(np.load(p)['y']) for p in ds.paths]\n",
        "print(\"Total:\", len(ds), \"| class0:\", labels.count(0), \"| class1:\", labels.count(1))\n",
        "\n",
        "# stratified 70/15/15 split\n",
        "idx0 = [i for i,y in enumerate(labels) if y==0]\n",
        "idx1 = [i for i,y in enumerate(labels) if y==1]\n",
        "def split_idx(idxs, frac_tr=0.7, frac_va=0.15):\n",
        "    idxs = idxs[:]; random.Random(SEED).shuffle(idxs)\n",
        "    n=len(idxs); ntr=int(n*frac_tr); nva=int(n*frac_va)\n",
        "    return idxs[:ntr], idxs[ntr:ntr+nva], idxs[ntr+nva:]\n",
        "tr0,va0,te0 = split_idx(idx0); tr1,va1,te1 = split_idx(idx1)\n",
        "\n",
        "train_idx = tr0 + tr1; val_idx = va0 + va1; test_idx = te0 + te1\n",
        "random.shuffle(train_idx); random.shuffle(val_idx); random.shuffle(test_idx)\n",
        "print(f\"Split → train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80LA2bWvqeyi",
        "outputId": "bc290b56-f9be-457a-b945-b2b9c3f7bfc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 95 | class0: 48 | class1: 47\n",
            "Split → train=65 val=14 test=16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# balanced sampler (train)\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "class_count = np.array([train_labels.count(0), train_labels.count(1)])\n",
        "w = 1.0 / np.where(class_count==0, 1, class_count)\n",
        "sample_weights = np.array([w[l] for l in train_labels])\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_dl = DataLoader(Subset(ds, train_idx), batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_dl   = DataLoader(Subset(ds, val_idx),   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dl  = DataLoader(Subset(ds, test_idx),  batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "rUloQpjqqiUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.networks.nets import DenseNet121\n",
        "\n",
        "model = DenseNet121(\n",
        "    spatial_dims=3,\n",
        "    in_channels=1,\n",
        "    out_channels=2\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "_bKQD6R9qm2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- লস/অপ্টিম/স্কেজিউলার ----------\n",
        "n0, n1 = labels.count(0), labels.count(1)\n",
        "w0 = 1.0 if n0==0 else (len(labels)/(2.0*n0))\n",
        "w1 = 1.0 if n1==0 else (len(labels)/(2.0*n1))\n",
        "class_weights = torch.tensor([w0, w1], device=device, dtype=torch.float32)\n",
        "\n",
        "crit = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
        "opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=4)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5EAD2Zfq6fe",
        "outputId": "7ec1acda-7522-4a83-b4d2-27bcc627c128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2756800604.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=torch.cuda.is_available())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- অগমেন্টেশন ----------\n",
        "def augment3d(x):  # x: (B,1,D,H,W)\n",
        "    import random, torch\n",
        "    if random.random()<0.5: x = torch.flip(x, [2])  # Z flip\n",
        "    if random.random()<0.5: x = torch.flip(x, [3])  # Y flip\n",
        "    if random.random()<0.5: x = torch.flip(x, [4])  # X flip\n",
        "    if random.random()<0.3: x = x.transpose(2,3)    # 90° Z<->Y\n",
        "    if random.random()<0.3: x = x.transpose(3,4)    # 90° Y<->X"
      ],
      "metadata": {
        "id": "h-K2AjfhsH2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- ট্রেন/ভ্যাল লুপ ----------\n",
        "def run_epoch(dl, train=True):\n",
        "    model.train(train)\n",
        "    total = correct = 0\n",
        "    loss_sum = 0.0\n",
        "    for x,y in dl:\n",
        "        if train:\n",
        "            x = augment3d(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss   = crit(logits, y)\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        loss_sum += float(loss.detach().item()) * x.size(0)\n",
        "        correct  += (logits.argmax(1)==y).sum().item()\n",
        "        total    += x.size(0)\n",
        "    return loss_sum/max(1,total), correct/max(1,total)"
      ],
      "metadata": {
        "id": "fctilUp9sLDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random\n",
        "\n",
        "# 0) CPU হলে pin_memory=False করে দিন\n",
        "PIN_MEMORY = torch.cuda.is_available()\n",
        "\n",
        "# (ঐচ্ছিক) যদি DataLoader আগে বানানো থাকে, নতুন করে বানান\n",
        "# train_dl = DataLoader(Subset(ds, train_idx), batch_size=BATCH_SIZE,\n",
        "#                       sampler=sampler, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "# val_dl   = DataLoader(Subset(ds, val_idx),   batch_size=BATCH_SIZE,\n",
        "#                       shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "# test_dl  = DataLoader(Subset(ds, test_idx),  batch_size=BATCH_SIZE,\n",
        "#                       shuffle=False, num_workers=2, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# 1) স্যানিটি-চেক: এক ব্যাচ এনে টাইপ/শেপ দেখুন\n",
        "xb, yb = next(iter(train_dl))\n",
        "print(\"Sample batch types:\", type(xb), type(yb))\n",
        "print(\"Sample batch shapes:\", xb.shape, yb.shape)\n",
        "assert xb is not None and yb is not None, \"❌ DataLoader returned None\"\n",
        "assert isinstance(xb, torch.Tensor) and isinstance(yb, torch.Tensor), \"❌ Not tensors\"\n",
        "\n",
        "# 2) সেফ অগমেন্টেশন: সব পথে নিশ্চিতভাবে `x` ফেরত দেয়\n",
        "def augment3d(x: torch.Tensor) -> torch.Tensor:\n",
        "    # ইনপুট চেক\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        raise TypeError(\"augment3d expects a torch.Tensor\")\n",
        "    if x.ndim != 5:\n",
        "        raise ValueError(f\"Expected (B, C, D, H, W). Got shape={tuple(x.shape)}\")\n",
        "\n",
        "    # ফ্লিপ\n",
        "    if random.random() < 0.5: x = torch.flip(x, [2])  # D\n",
        "    if random.random() < 0.5: x = torch.flip(x, [3])  # H\n",
        "    if random.random() < 0.5: x = torch.flip(x, [4])  # W\n",
        "    # 90° রোটেশন (axis swap)\n",
        "    if random.random() < 0.3: x = x.transpose(2,3)    # D<->H\n",
        "    if random.random() < 0.3: x = x.transpose(3,4)    # H<->W\n",
        "\n",
        "    # কনটিগুয়াস টেনসর (কিছু মডিউল non-contiguous পছন্দ করে না)\n",
        "    return x.contiguous()\n",
        "\n",
        "# 3) সঠিক run_epoch: সবসময় (avg_loss, acc) রিটার্ন করে\n",
        "from torch.cuda.amp import autocast\n",
        "# PyTorch >=2.0 হলে:\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "def run_epoch(dl, train: bool = True):\n",
        "    model.train(train)\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    for x, y in dl:\n",
        "        # স্যানিটি\n",
        "        if x is None or y is None:\n",
        "            raise RuntimeError(\"Got None from DataLoader; check augment/run_epoch definitions.\")\n",
        "        if train:\n",
        "            x = augment3d(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "        loss_sum += float(loss.detach().item()) * x.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    avg_loss = loss_sum / max(total, 1)\n",
        "    acc = correct / max(total, 1)\n",
        "    return avg_loss, acc\n",
        "\n",
        "print(\"✅ augment3d & run_epoch redefined safely.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z7yiHgstNSw",
        "outputId": "18d9b19b-8d68-4dca-800d-7a700d41c534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample batch types: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "Sample batch shapes: torch.Size([2, 1, 128, 128, 128]) torch.Size([2])\n",
            "✅ augment3d & run_epoch redefined safely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyYMm2mcP05g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Lightweight 3D CNN Training Script ====\n",
        "import os, glob, random, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
        "\n",
        "# ---------- Config ----------\n",
        "DATA_DIR     = \"/content/preproc\"   # আপনার .npz ফোল্ডার\n",
        "BATCH_SIZE   = 2                    # GPU কম হলে 1 দিন\n",
        "EPOCHS       = 60                   # early stopping আছে\n",
        "LR           = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "SEED         = 42\n",
        "PATIENCE     = 10\n",
        "CKPT         = \"/content/best_light3dcnn.pt\"\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN_MEMORY = torch.cuda.is_available()\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class NPZ3DDataset(Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
        "        assert len(self.paths)>0, \"No .npz found in DATA_DIR!\"\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        d = np.load(self.paths[idx])\n",
        "        x = torch.from_numpy(d[\"x\"]).float()         # (1, D, H, W) in [0,1]\n",
        "        y = torch.tensor(int(d[\"y\"]), dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "ds = NPZ3DDataset(DATA_DIR)\n",
        "labels = [int(np.load(p)[\"y\"]) for p in ds.paths]\n",
        "print(\"Total:\", len(ds), \"| class0:\", labels.count(0), \"| class1:\", labels.count(1))\n",
        "\n",
        "# ---------- Stratified split ----------\n",
        "idx0 = [i for i,y in enumerate(labels) if y==0]\n",
        "idx1 = [i for i,y in enumerate(labels) if y==1]\n",
        "def split_idx(idxs, frac_tr=0.7, frac_va=0.15):\n",
        "    idxs = idxs[:]; random.Random(SEED).shuffle(idxs)\n",
        "    n=len(idxs); ntr=int(n*frac_tr); nva=int(n*frac_va)\n",
        "    return idxs[:ntr], idxs[ntr:ntr+nva], idxs[ntr+nva:]\n",
        "tr0,va0,te0 = split_idx(idx0); tr1,va1,te1 = split_idx(idx1)\n",
        "train_idx = tr0+tr1; val_idx = va0+va1; test_idx = te0+te1\n",
        "random.shuffle(train_idx); random.shuffle(val_idx); random.shuffle(test_idx)\n",
        "print(f\"Split → train={len(train_idx)}  val={len(val_idx)}  test={len(test_idx)}\")\n",
        "\n",
        "# ---------- Loaders (balanced sampler for train) ----------\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "class_count  = np.array([train_labels.count(0), train_labels.count(1)])\n",
        "w = 1.0 / np.where(class_count==0, 1, class_count)\n",
        "sample_weights = np.array([w[l] for l in train_labels], dtype=np.float64)\n",
        "sampler   = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_dl = DataLoader(Subset(ds, train_idx), batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                      num_workers=2, pin_memory=PIN_MEMORY)\n",
        "val_dl   = DataLoader(Subset(ds, val_idx),   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                      num_workers=2, pin_memory=PIN_MEMORY)\n",
        "test_dl  = DataLoader(Subset(ds, test_idx),  batch_size=BATCH_SIZE, shuffle=False,\n",
        "                      num_workers=2, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# ---------- Lightweight 3D CNN ----------\n",
        "class Light3DCNN(nn.Module):\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        def block(cin, cout):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv3d(cin, cout, 3, padding=1),\n",
        "                nn.BatchNorm3d(cout),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv3d(cout, cout, 3, padding=1),\n",
        "                nn.BatchNorm3d(cout),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool3d(2)\n",
        "            )\n",
        "        self.features = nn.Sequential(\n",
        "            block(1, 16),   # /2\n",
        "            block(16, 32),  # /4\n",
        "            block(32, 64),  # /8\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = Light3DCNN().to(device)\n",
        "\n",
        "# ---------- Loss / Optim / Scheduler ----------\n",
        "n0, n1 = labels.count(0), labels.count(1)\n",
        "w0 = 1.0 if n0==0 else (len(labels)/(2.0*n0))\n",
        "w1 = 1.0 if n1==0 else (len(labels)/(2.0*n1))\n",
        "class_weights = torch.tensor([w0, w1], device=device, dtype=torch.float32)\n",
        "\n",
        "crit = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
        "opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=4)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# ---------- Augmentation ----------\n",
        "def augment3d(x: torch.Tensor) -> torch.Tensor:\n",
        "    if random.random()<0.5: x = torch.flip(x, [2])  # D\n",
        "    if random.random()<0.5: x = torch.flip(x, [3])  # H\n",
        "    if random.random()<0.5: x = torch.flip(x, [4])  # W\n",
        "    if random.random()<0.3: x = x.transpose(2,3)\n",
        "    if random.random()<0.3: x = x.transpose(3,4)\n",
        "    return x.contiguous()\n",
        "\n",
        "# ---------- Train/Val loop ----------\n",
        "def run_epoch(dl, train=True):\n",
        "    model.train(train)\n",
        "    total = correct = 0\n",
        "    loss_sum = 0.0\n",
        "    for x,y in dl:\n",
        "        if train:\n",
        "            x = augment3d(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss   = crit(logits, y)\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "        loss_sum += float(loss.detach().item()) * x.size(0)\n",
        "        correct  += (logits.argmax(1)==y).sum().item()\n",
        "        total    += x.size(0)\n",
        "\n",
        "    return loss_sum/max(1,total), correct/max(1,total)\n",
        "\n",
        "# ---------- Training ----------\n",
        "best_val_acc, best_val_loss, patience = 0.0, float(\"inf\"), 0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_dl, True)\n",
        "    va_loss, va_acc = run_epoch(val_dl, False)\n",
        "\n",
        "    scheduler.step(va_loss)\n",
        "\n",
        "    improved = (va_acc > best_val_acc) or (va_acc == best_val_acc and va_loss < best_val_loss)\n",
        "    if improved:\n",
        "        best_val_acc, best_val_loss = va_acc, va_loss\n",
        "        torch.save(model.state_dict(), CKPT)\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train {tr_loss:.4f}/{tr_acc:.3f} \"\n",
        "          f\"| val {va_loss:.4f}/{va_acc:.3f} | best_val_acc {best_val_acc:.3f}\")\n",
        "\n",
        "    if patience >= PATIENCE:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n",
        "\n",
        "# ---------- Test ----------\n",
        "model.load_state_dict(torch.load(CKPT, map_location=device))\n",
        "model.eval()\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_dl:\n",
        "        x = x.to(device)\n",
        "        pred = model(x).argmax(1).cpu().numpy()\n",
        "        all_p.extend(pred.tolist()); all_y.extend(y.numpy().tolist())\n",
        "\n",
        "all_y = np.array(all_y); all_p = np.array(all_p)\n",
        "acc = (all_y == all_p).mean()\n",
        "cm = np.zeros((2,2), dtype=int)  # [[TN FP],[FN TP]]\n",
        "for yt, yp in zip(all_y, all_p): cm[yt, yp] += 1\n",
        "\n",
        "print(f\"\\nTEST ACCURACY: {acc:.3f}\")\n",
        "print(\"Confusion Matrix [[TN FP],[FN TP]]:\\n\", cm)\n",
        "print(\"Unique predictions:\", sorted(set(all_p)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUaQ7e2LzK1o",
        "outputId": "b3af4c69-0fed-4150-f686-70e3ae242a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Total: 95 | class0: 48 | class1: 47\n",
            "Split → train=65  val=14  test=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2410657665.py:105: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
            "/tmp/ipython-input-2410657665.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train 0.7434/0.492 | val 0.6917/0.571 | best_val_acc 0.571\n",
            "Epoch 02 | train 0.6995/0.477 | val 0.7132/0.500 | best_val_acc 0.571\n",
            "Epoch 03 | train 0.7055/0.446 | val 0.7105/0.500 | best_val_acc 0.571\n",
            "Epoch 04 | train 0.7088/0.446 | val 0.6822/0.571 | best_val_acc 0.571\n",
            "Epoch 05 | train 0.7085/0.462 | val 0.6690/0.500 | best_val_acc 0.571\n",
            "Epoch 06 | train 0.6803/0.569 | val 0.6687/0.500 | best_val_acc 0.571\n",
            "Epoch 07 | train 0.6883/0.569 | val 0.6605/0.571 | best_val_acc 0.571\n",
            "Epoch 08 | train 0.6938/0.554 | val 0.6608/0.571 | best_val_acc 0.571\n",
            "Epoch 09 | train 0.7152/0.462 | val 0.6694/0.500 | best_val_acc 0.571\n",
            "Epoch 10 | train 0.6873/0.585 | val 0.6633/0.714 | best_val_acc 0.714\n",
            "Epoch 11 | train 0.6686/0.585 | val 0.6771/0.500 | best_val_acc 0.714\n",
            "Epoch 12 | train 0.7079/0.538 | val 0.6615/0.571 | best_val_acc 0.714\n",
            "Epoch 13 | train 0.6879/0.492 | val 0.6633/0.714 | best_val_acc 0.714\n",
            "Epoch 14 | train 0.6761/0.631 | val 0.6720/0.714 | best_val_acc 0.714\n",
            "Epoch 15 | train 0.7043/0.446 | val 0.6807/0.643 | best_val_acc 0.714\n",
            "Epoch 16 | train 0.7058/0.492 | val 0.6877/0.643 | best_val_acc 0.714\n",
            "Epoch 17 | train 0.6947/0.508 | val 0.6810/0.500 | best_val_acc 0.714\n",
            "Epoch 18 | train 0.6887/0.569 | val 0.6787/0.500 | best_val_acc 0.714\n",
            "Epoch 19 | train 0.6933/0.585 | val 0.6738/0.500 | best_val_acc 0.714\n",
            "Epoch 20 | train 0.6752/0.585 | val 0.6681/0.500 | best_val_acc 0.714\n",
            "Early stopping.\n",
            "\n",
            "TEST ACCURACY: 0.688\n",
            "Confusion Matrix [[TN FP],[FN TP]]:\n",
            " [[5 3]\n",
            " [2 6]]\n",
            "Unique predictions: [np.int64(0), np.int64(1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "DEkSYHYBICoF",
        "outputId": "54e0b943-3b26-4384-f36a-4133fdfcceb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1823377705.py, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1823377705.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    META_CSV   =    /content/metadata.csv\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "META_CSV = \"/content/metadata.csv\"  # আপনার আপলোড করা ফাইলের নাম/পাথ\n",
        "import os\n",
        "print(\"Found?\" , os.path.exists(META_CSV))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kHAVMDrJX8U",
        "outputId": "a780ea6d-bb03-477e-940f-bea5f7a20ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta = pd.read_csv(META_CSV)\n",
        "print(meta.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uruXRvxUJ-FM",
        "outputId": "424ccb1b-33b3-4e51-daf0-95fec52070ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   patient_id  label  age smoking_status\n",
            "0          10      1   74         former\n",
            "1          28      1   47         former\n",
            "2           1      0   42         smoker\n",
            "3          53      1   51         smoker\n",
            "4          38      0   58         smoker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Image (3D CNN 64D) + Metadata Fusion → Random Forest ======\n",
        "!pip -q install scikit-learn\n",
        "\n",
        "import os, glob, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "DATA_DIR   = \"/content/preproc\"          # আপনার npz dataset folder\n",
        "META_CSV   = \"/content/metadata.csv\"     # আপনার uploaded metadata file\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS_ENC = 20\n",
        "LR         = 1e-4\n",
        "SEED       = 42\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class NPZ3D(Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
        "        assert self.paths, \"No .npz found!\"\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]; d = np.load(p)\n",
        "        x = torch.from_numpy(d[\"x\"]).float()\n",
        "        y = torch.tensor(int(d[\"y\"]), dtype=torch.long)\n",
        "        stem = os.path.splitext(os.path.basename(p))[0]\n",
        "        try: npz_id = int(stem)  # e.g. 0010.npz -> 10\n",
        "        except: npz_id = stem\n",
        "        return x, y, npz_id\n",
        "\n",
        "full_ds = NPZ3D(DATA_DIR)\n",
        "labels = [int(np.load(p)['y']) for p in full_ds.paths]\n",
        "print(f\"Total {len(full_ds)} | class0={labels.count(0)} class1={labels.count(1)}\")\n",
        "\n",
        "# ---------------- Split ----------------\n",
        "idx0=[i for i,y in enumerate(labels) if y==0]\n",
        "idx1=[i for i,y in enumerate(labels) if y==1]\n",
        "def split(idxs):\n",
        "    random.Random(SEED).shuffle(idxs); n=len(idxs)\n",
        "    return idxs[:int(0.7*n)], idxs[int(0.7*n):int(0.85*n)], idxs[int(0.85*n):]\n",
        "tr0,va0,te0=split(idx0); tr1,va1,te1=split(idx1)\n",
        "train_idx=tr0+tr1; val_idx=va0+va1; test_idx=te0+te1\n",
        "\n",
        "def subset(ds, idxs):\n",
        "    class _S(Dataset):\n",
        "        def __init__(self, base, sel): self.base, self.sel=base, sel\n",
        "        def __len__(self): return len(self.sel)\n",
        "        def __getitem__(self,i): return self.base[self.sel[i]]\n",
        "    return _S(ds, idxs)\n",
        "\n",
        "train_dl = DataLoader(subset(full_ds,train_idx), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl   = DataLoader(subset(full_ds,val_idx),   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dl  = DataLoader(subset(full_ds,test_idx),  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ---------------- Light 3D Encoder ----------------\n",
        "class Light3DEncoder(nn.Module):\n",
        "    def __init__(self, out_ch=64):\n",
        "        super().__init__()\n",
        "        def blk(cin,cout):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv3d(cin, cout, 3,padding=1), nn.BatchNorm3d(cout), nn.ReLU(),\n",
        "                nn.Conv3d(cout, cout,3,padding=1), nn.BatchNorm3d(cout), nn.ReLU(),\n",
        "                nn.MaxPool3d(2))\n",
        "        self.body=nn.Sequential(blk(1,16), blk(16,32), blk(32,out_ch))\n",
        "        self.pool=nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc  = nn.Linear(out_ch,2)\n",
        "    def forward(self,x,feat_only=False):\n",
        "        z=self.body(x); z=self.pool(z).flatten(1)\n",
        "        if feat_only: return z\n",
        "        return self.fc(z)\n",
        "\n",
        "enc = Light3DEncoder(64).to(device)\n",
        "opt = torch.optim.AdamW(enc.parameters(), lr=LR)\n",
        "crit= nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(dl, train=True):\n",
        "    enc.train(train)\n",
        "    tot=0; corr=0; loss_sum=0.0\n",
        "    for x,y,_ in dl:\n",
        "        if train: opt.zero_grad(set_to_none=True)\n",
        "        x,y=x.to(device), y.to(device)\n",
        "        logits=enc(x)\n",
        "        loss=crit(logits,y)\n",
        "        if train: loss.backward(); opt.step()\n",
        "        loss_sum+=float(loss.item())*x.size(0)\n",
        "        corr+=(logits.argmax(1)==y).sum().item(); tot+=x.size(0)\n",
        "    return loss_sum/max(1,tot), corr/max(1,tot)\n",
        "\n",
        "print(\">> Warm-up encoder...\")\n",
        "best_va=0; pat=0\n",
        "for ep in range(1,EPOCHS_ENC+1):\n",
        "    tr_loss,tr_acc=run_epoch(train_dl,True)\n",
        "    va_loss,va_acc=run_epoch(val_dl,False)\n",
        "    if va_acc>best_va:\n",
        "        best_va=va_acc; pat=0\n",
        "        torch.save(enc.state_dict(),\"/content/enc_best.pt\")\n",
        "    else:\n",
        "        pat+=1\n",
        "    print(f\"Ep{ep:02d} | tr {tr_loss:.3f}/{tr_acc:.3f} | va {va_loss:.3f}/{va_acc:.3f} | best {best_va:.3f}\")\n",
        "    if pat>=5: break\n",
        "\n",
        "enc.load_state_dict(torch.load(\"/content/enc_best.pt\", map_location=device))\n",
        "\n",
        "# ---------------- Extract Features ----------------\n",
        "def extract(dl):\n",
        "    enc.eval(); X=[]; Y=[]; IDs=[]\n",
        "    with torch.no_grad():\n",
        "        for x,y,i in dl:\n",
        "            x=x.to(device); z=enc(x,feat_only=True)\n",
        "            X.append(z.cpu().numpy()); Y+=y.numpy().tolist(); IDs+=list(i)\n",
        "    return np.concatenate(X,0), np.array(Y), np.array(IDs)\n",
        "\n",
        "Xtr,ytr,idtr=extract(train_dl); Xva,yva,idva=extract(val_dl); Xte,yte,idte=extract(test_dl)\n",
        "print(\"Image feature shape:\", Xtr.shape)\n",
        "\n",
        "# ---------------- Metadata Processing ----------------\n",
        "meta = pd.read_csv(META_CSV)\n",
        "assert \"patient_id\" in meta.columns, \"metadata.csv must have patient_id column\"\n",
        "\n",
        "# age clean\n",
        "if \"age\" in meta.columns:\n",
        "    def parse_age(v):\n",
        "        try: return float(\"\".join(ch for ch in str(v) if ch.isdigit()))\n",
        "        except: return np.nan\n",
        "    meta[\"age\"] = meta[\"age\"].apply(parse_age)\n",
        "\n",
        "num_cols = [c for c in [\"age\"] if c in meta.columns]\n",
        "cat_cols = [c for c in [\"smoking_status\",\"gender\",\"family_history\"] if c in meta.columns]\n",
        "\n",
        "def make_meta_matrix(ids):\n",
        "    rows=[]\n",
        "    for i in ids:\n",
        "        row = meta[meta[\"patient_id\"]==i]\n",
        "        if row.empty:\n",
        "            rows.append(np.zeros(len(num_cols)+len(cat_cols)))\n",
        "        else:\n",
        "            vals=[]\n",
        "            if num_cols:\n",
        "                vals += row[num_cols].iloc[0].astype(float).fillna(0).values.tolist()\n",
        "            if cat_cols:\n",
        "                vals += row[cat_cols].iloc[0].astype(str).values.tolist()\n",
        "            rows.append(vals)\n",
        "    df = pd.DataFrame(rows, columns=num_cols+cat_cols)\n",
        "    if cat_cols:\n",
        "        for c in cat_cols: df[c]=df[c].astype(str).fillna(\"UNK\")\n",
        "        oh = pd.get_dummies(df[cat_cols])\n",
        "        df = pd.concat([df[num_cols], oh], axis=1)\n",
        "    else:\n",
        "        df = df[num_cols]\n",
        "    return df.fillna(0).to_numpy(np.float32)\n",
        "\n",
        "Xtr_meta=make_meta_matrix(idtr)\n",
        "Xva_meta=make_meta_matrix(idva)\n",
        "Xte_meta=make_meta_matrix(idte)\n",
        "print(\"Meta dims:\", Xtr_meta.shape[1])\n",
        "\n",
        "# ---------------- Concatenate ----------------\n",
        "Xtr_f=np.concatenate([Xtr,Xtr_meta],1)\n",
        "Xva_f=np.concatenate([Xva,Xva_meta],1)\n",
        "Xte_f=np.concatenate([Xte,Xte_meta],1)\n",
        "print(\"Final feature dims:\", Xtr_f.shape[1])\n",
        "\n",
        "# ---------------- Random Forest ----------------\n",
        "rf=RandomForestClassifier(n_estimators=400, class_weight=\"balanced\", random_state=SEED)\n",
        "rf.fit(np.vstack([Xtr_f,Xva_f]), np.hstack([ytr,yva]))\n",
        "\n",
        "pred=rf.predict(Xte_f)\n",
        "acc=accuracy_score(yte,pred); cm=confusion_matrix(yte,pred); f1=f1_score(yte,pred)\n",
        "print(f\"\\nTEST ACC: {acc:.3f} | F1={f1:.3f}\")\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\",cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZgI26ulL6po",
        "outputId": "b575ebac-18c6-44a7-dfb3-85538877eeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Total 95 | class0=48 class1=47\n",
            ">> Warm-up encoder...\n",
            "Ep01 | tr 0.753/0.477 | va 0.739/0.500 | best 0.500\n",
            "Ep02 | tr 0.716/0.477 | va 0.673/0.500 | best 0.500\n",
            "Ep03 | tr 0.699/0.600 | va 0.706/0.357 | best 0.500\n",
            "Ep04 | tr 0.702/0.523 | va 0.688/0.500 | best 0.500\n",
            "Ep05 | tr 0.694/0.554 | va 0.702/0.500 | best 0.500\n",
            "Ep06 | tr 0.684/0.600 | va 0.664/0.571 | best 0.571\n",
            "Ep07 | tr 0.672/0.600 | va 0.663/0.500 | best 0.571\n",
            "Ep08 | tr 0.687/0.631 | va 0.673/0.500 | best 0.571\n",
            "Ep09 | tr 0.693/0.569 | va 0.674/0.500 | best 0.571\n",
            "Ep10 | tr 0.678/0.615 | va 0.651/0.500 | best 0.571\n",
            "Ep11 | tr 0.695/0.554 | va 0.702/0.500 | best 0.571\n",
            "Image feature shape: (65, 64)\n",
            "Meta dims: 4\n",
            "Final feature dims: 68\n",
            "\n",
            "TEST ACC: 0.562 | F1=0.533\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[5 3]\n",
            " [4 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Final Fusion Pipeline: 3D image (64D) + metadata → RF / LogReg / SVM =====\n",
        "!pip -q install scikit-learn torchvision\n",
        "\n",
        "import os, glob, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "DATA_DIR   = \"/content/preproc\"          # .npz folder (x: (1,D,H,W), y: 0/1)\n",
        "META_CSV   = \"/content/metadata.csv\"     # metadata file (must have 'patient_id')\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS_ENC = 35                          # encoder warm-up epochs (আগে 20 ছিল)\n",
        "LR         = 1e-4\n",
        "SEED       = 42\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class NPZ3D(Dataset):\n",
        "    def __init__(self, folder):\n",
        "        self.paths = sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
        "        assert self.paths, f\"No .npz found in {folder}\"\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        d = np.load(p)\n",
        "        x = torch.from_numpy(d[\"x\"]).float()\n",
        "        y = torch.tensor(int(d[\"y\"]), dtype=torch.long)\n",
        "        stem = os.path.splitext(os.path.basename(p))[0]\n",
        "        try: npz_id = int(stem)       # e.g. 00012.npz → 12\n",
        "        except: npz_id = stem\n",
        "        return x, y, npz_id\n",
        "\n",
        "full_ds = NPZ3D(DATA_DIR)\n",
        "labels = [int(np.load(p)['y']) for p in full_ds.paths]\n",
        "print(f\"Total {len(full_ds)} | class0={labels.count(0)} class1={labels.count(1)}\")\n",
        "\n",
        "# ---------------- Stratified split (70/15/15) ----------------\n",
        "idx0=[i for i,y in enumerate(labels) if y==0]\n",
        "idx1=[i for i,y in enumerate(labels) if y==1]\n",
        "def split(idxs):\n",
        "    idxs = idxs[:]; random.Random(SEED).shuffle(idxs); n=len(idxs)\n",
        "    return idxs[:int(0.7*n)], idxs[int(0.7*n):int(0.85*n)], idxs[int(0.85*n):]\n",
        "tr0,va0,te0=split(idx0); tr1,va1,te1=split(idx1)\n",
        "train_idx=tr0+tr1; val_idx=va0+va1; test_idx=te0+te1\n",
        "\n",
        "def subset(ds, idxs):\n",
        "    class _S(Dataset):\n",
        "        def __init__(self, base, sel): self.base, self.sel=base, sel\n",
        "        def __len__(self): return len(self.sel)\n",
        "        def __getitem__(self,i): return self.base[self.sel[i]]\n",
        "    return _S(ds, idxs)\n",
        "\n",
        "train_dl = DataLoader(subset(full_ds,train_idx), batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "val_dl   = DataLoader(subset(full_ds,val_idx),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "test_dl  = DataLoader(subset(full_ds,test_idx),  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# ---------------- 3D Encoder (Light) + Augmentation ----------------\n",
        "class Light3DEncoder(nn.Module):\n",
        "    def __init__(self, out_ch=64):\n",
        "        super().__init__()\n",
        "        def blk(cin, cout):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv3d(cin, cout, 3, padding=1), nn.BatchNorm3d(cout), nn.ReLU(inplace=True),\n",
        "                nn.Conv3d(cout, cout, 3, padding=1), nn.BatchNorm3d(cout), nn.ReLU(inplace=True),\n",
        "                nn.MaxPool3d(2)\n",
        "            )\n",
        "        self.body = nn.Sequential(blk(1,16), blk(16,32), blk(32,out_ch))  # /2,/4,/8\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc   = nn.Linear(out_ch, 2)   # warm-up head\n",
        "    def forward(self, x, feat_only=False):\n",
        "        z = self.body(x)\n",
        "        z = self.pool(z).flatten(1)        # (B, out_ch)\n",
        "        if feat_only: return z\n",
        "        return self.fc(z)\n",
        "\n",
        "def augment3d_light(x: torch.Tensor) -> torch.Tensor:\n",
        "    # simple flips + axis swap + light noise\n",
        "    if random.random()<0.5: x = torch.flip(x, [2])  # D\n",
        "    if random.random()<0.5: x = torch.flip(x, [3])  # H\n",
        "    if random.random()<0.5: x = torch.flip(x, [4])  # W\n",
        "    if random.random()<0.3: x = x.transpose(2,3)\n",
        "    if random.random()<0.3: x = x.transpose(3,4)\n",
        "    if random.random()<0.5:\n",
        "        noise = torch.randn_like(x)*0.02\n",
        "        x = (x + noise).clamp(0,1)\n",
        "    return x.contiguous()\n",
        "\n",
        "enc = Light3DEncoder(64).to(device)\n",
        "opt = torch.optim.AdamW(enc.parameters(), lr=LR, weight_decay=1e-4)\n",
        "crit= nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(dl, train=True):\n",
        "    enc.train(train)\n",
        "    tot=corr=0; loss_sum=0.0\n",
        "    for x,y,_ in dl:\n",
        "        if train:\n",
        "            x = augment3d_light(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        logits = enc(x)\n",
        "        loss   = crit(logits, y)\n",
        "        if train:\n",
        "            loss.backward(); opt.step()\n",
        "        loss_sum += float(loss.item()) * x.size(0)\n",
        "        corr     += (logits.argmax(1)==y).sum().item()\n",
        "        tot      += x.size(0)\n",
        "    return loss_sum/max(1,tot), corr/max(1,tot)\n",
        "\n",
        "print(\">> Warm-up encoder…\")\n",
        "best_va=0.0; pat=0; PATIENCE=6\n",
        "for ep in range(1, EPOCHS_ENC+1):\n",
        "    tr_loss,tr_acc = run_epoch(train_dl, True)\n",
        "    va_loss,va_acc = run_epoch(val_dl,   False)\n",
        "    if va_acc>best_va:\n",
        "        best_va=va_acc; pat=0\n",
        "        torch.save(enc.state_dict(), \"/content/enc_best.pt\")\n",
        "    else:\n",
        "        pat+=1\n",
        "    print(f\"Ep{ep:02d} | tr {tr_loss:.3f}/{tr_acc:.3f} | va {va_loss:.3f}/{va_acc:.3f} | best {best_va:.3f}\")\n",
        "    if pat>=PATIENCE:\n",
        "        print(\"Early stop.\"); break\n",
        "\n",
        "enc.load_state_dict(torch.load(\"/content/enc_best.pt\", map_location=device))\n",
        "\n",
        "# ---------------- Extract 64D image features ----------------\n",
        "def extract(dl):\n",
        "    enc.eval(); X=[]; Y=[]; IDs=[]\n",
        "    with torch.no_grad():\n",
        "        for x,y,i in dl:\n",
        "            x = x.to(device)\n",
        "            z = enc(x, feat_only=True)     # (B,64)\n",
        "            X.append(z.cpu().numpy()); Y+=y.numpy().tolist(); IDs+=list(i)\n",
        "    return np.concatenate(X,0), np.array(Y), np.array(IDs)\n",
        "\n",
        "Xtr_img,ytr,idtr = extract(train_dl)\n",
        "Xva_img,yva,idva = extract(val_dl)\n",
        "Xte_img,yte,idte = extract(test_dl)\n",
        "print(\"Image feature shapes:\", Xtr_img.shape, Xva_img.shape, Xte_img.shape)\n",
        "\n",
        "# ---------------- Metadata: read + clean + one-hot ----------------\n",
        "meta = pd.read_csv(META_CSV)\n",
        "assert \"patient_id\" in meta.columns, \"metadata.csv must have 'patient_id' column\"\n",
        "\n",
        "# helper to intify id if possible\n",
        "def to_int_maybe(v):\n",
        "    try: return int(str(v))\n",
        "    except: return np.nan\n",
        "meta[\"_pid_int\"] = meta[\"patient_id\"].apply(to_int_maybe)\n",
        "if \"age\" in meta.columns:\n",
        "    def parse_age(v):\n",
        "        s = str(v); dig = \"\".join(ch for ch in s if ch.isdigit())\n",
        "        return float(dig) if dig else np.nan\n",
        "    meta[\"age\"] = meta[\"age\"].apply(parse_age)\n",
        "\n",
        "num_cols = [c for c in [\"age\"] if c in meta.columns]\n",
        "cat_cols = [c for c in [\"smoking_status\",\"gender\",\"family_history\"] if c in meta.columns]\n",
        "\n",
        "def rows_for_ids(ids):\n",
        "    rows=[]\n",
        "    for rid in ids:\n",
        "        r = meta[meta[\"_pid_int\"]==rid]\n",
        "        if r.empty:\n",
        "            # fallback exact string match\n",
        "            r = meta[ meta[\"patient_id\"].astype(str) == str(rid) ]\n",
        "        rows.append(r.iloc[0:1].copy() if not r.empty else pd.DataFrame(columns=meta.columns))\n",
        "    return rows\n",
        "\n",
        "def make_split_df(ids):\n",
        "    rows = rows_for_ids(ids)\n",
        "    out  = []\n",
        "    for r in rows:\n",
        "        if r.empty:\n",
        "            rec = {}\n",
        "        else:\n",
        "            rec = r.iloc[0].to_dict()\n",
        "        out.append(rec)\n",
        "    df = pd.DataFrame(out)\n",
        "    # keep only selected cols\n",
        "    keep = []\n",
        "    if num_cols: keep += num_cols\n",
        "    if cat_cols: keep += cat_cols\n",
        "    df = df[keep] if keep else pd.DataFrame(index=df.index)\n",
        "    # defaults\n",
        "    for c in num_cols: df[c] = pd.to_numeric(df.get(c, np.nan), errors=\"coerce\")\n",
        "    for c in cat_cols: df[c] = df.get(c, \"UNK\").astype(str)\n",
        "    return df\n",
        "\n",
        "df_tr = make_split_df(idtr)\n",
        "df_va = make_split_df(idva)\n",
        "df_te = make_split_df(idte)\n",
        "full  = pd.concat([df_tr, df_va, df_te], axis=0, ignore_index=True)\n",
        "\n",
        "# one-hot for cats; numeric fillna=median (simple)\n",
        "if num_cols:\n",
        "    for c in num_cols:\n",
        "        med = full[c].median(skipna=True) if c in full.columns else 0.0\n",
        "        full[c] = pd.to_numeric(full.get(c, med), errors=\"coerce\").fillna(med)\n",
        "num_part = full[num_cols] if num_cols else pd.DataFrame(index=full.index)\n",
        "if cat_cols:\n",
        "    for c in cat_cols:\n",
        "        full[c] = full[c].fillna(\"UNK\").astype(str)\n",
        "    oh = pd.get_dummies(full[cat_cols], drop_first=False)\n",
        "else:\n",
        "    oh = pd.DataFrame(index=full.index)\n",
        "\n",
        "fullX = pd.concat([num_part, oh], axis=1).astype(np.float32)\n",
        "n_tr, n_va = len(df_tr), len(df_va)\n",
        "Xtr_meta = fullX.iloc[:n_tr].to_numpy()\n",
        "Xva_meta = fullX.iloc[n_tr:n_tr+n_va].to_numpy()\n",
        "Xte_meta = fullX.iloc[n_tr+n_va:].to_numpy()\n",
        "print(\"Meta dims:\", Xtr_meta.shape[1])\n",
        "\n",
        "# ---------------- Fusion features ----------------\n",
        "Xtr_f = np.concatenate([Xtr_img, Xtr_meta], axis=1)\n",
        "Xva_f = np.concatenate([Xva_img, Xva_meta], axis=1)\n",
        "Xte_f = np.concatenate([Xte_img, Xte_meta], axis=1)\n",
        "print(\"Final feature dims:\", Xtr_f.shape[1])\n",
        "\n",
        "# ---------------- RF (train->val threshold tune -> retrain on train+val) ----------------\n",
        "rf_params = dict(\n",
        "    n_estimators=800,\n",
        "    max_depth=12,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 1) fit on train only → tune threshold on val\n",
        "rf1 = RandomForestClassifier(**rf_params)\n",
        "rf1.fit(Xtr_f, ytr)\n",
        "val_proba = rf1.predict_proba(Xva_f)[:,1]\n",
        "best_t, best_f1 = 0.5, -1.0\n",
        "for t in np.linspace(0.2, 0.8, 49):\n",
        "    pred = (val_proba >= t).astype(int)\n",
        "    f1   = f1_score(yva, pred)\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_t = f1, t\n",
        "print(f\"[RF] Best VAL threshold: {best_t:.3f} (F1={best_f1:.3f})\")\n",
        "\n",
        "# 2) retrain on train+val\n",
        "Xtrva_f = np.vstack([Xtr_f, Xva_f])\n",
        "ytrva   = np.hstack([ytr, yva])\n",
        "rf = RandomForestClassifier(**rf_params)\n",
        "rf.fit(Xtrva_f, ytrva)\n",
        "\n",
        "# 3) test with tuned threshold\n",
        "test_proba = rf.predict_proba(Xte_f)[:,1]\n",
        "rf_pred = (test_proba >= best_t).astype(int)\n",
        "rf_acc = accuracy_score(yte, rf_pred); rf_f1 = f1_score(yte, rf_pred); rf_cm = confusion_matrix(yte, rf_pred)\n",
        "print(f\"\\n[RF] TEST ACC: {rf_acc:.3f} | F1: {rf_f1:.3f} | thr={best_t:.3f}\")\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", rf_cm)\n",
        "\n",
        "# ---------------- Logistic Regression (scaled) ----------------\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtrva_s = scaler.fit_transform(Xtrva_f)\n",
        "Xte_s   = scaler.transform(Xte_f)\n",
        "\n",
        "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\")\n",
        "logreg.fit(Xtrva_s, ytrva)\n",
        "lr_pred = logreg.predict(Xte_s)\n",
        "lr_acc = accuracy_score(yte, lr_pred); lr_f1 = f1_score(yte, lr_pred); lr_cm = confusion_matrix(yte, lr_pred)\n",
        "print(f\"\\n[LogReg] TEST ACC: {lr_acc:.3f} | F1: {lr_f1:.3f}\")\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", lr_cm)\n",
        "\n",
        "# ---------------- Linear SVM (scaled) ----------------\n",
        "svm = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
        "svm.fit(Xtrva_s, ytrva)\n",
        "svm_pred = (svm.decision_function(Xte_s) >= 0).astype(int)\n",
        "svm_acc = accuracy_score(yte, svm_pred); svm_f1 = f1_score(yte, svm_pred); svm_cm = confusion_matrix(yte, svm_pred)\n",
        "print(f\"\\n[LinearSVM] TEST ACC: {svm_acc:.3f} | F1: {svm_f1:.3f}\")\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", svm_cm)\n",
        "\n",
        "# (ঐচ্ছিক) ছোট রিপোর্ট\n",
        "print(\"\\n[RF] classification report:\\n\", classification_report(yte, rf_pred, digits=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXotkxTQP2_T",
        "outputId": "78441eca-3d65-4e88-a274-4ef365f3d09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Total 95 | class0=48 class1=47\n",
            ">> Warm-up encoder…\n",
            "Ep01 | tr 0.754/0.477 | va 0.731/0.500 | best 0.500\n",
            "Ep02 | tr 0.708/0.462 | va 0.683/0.500 | best 0.500\n",
            "Ep03 | tr 0.704/0.492 | va 0.710/0.500 | best 0.500\n",
            "Ep04 | tr 0.709/0.569 | va 0.682/0.714 | best 0.714\n",
            "Ep05 | tr 0.682/0.585 | va 0.674/0.500 | best 0.714\n",
            "Ep06 | tr 0.697/0.492 | va 0.691/0.500 | best 0.714\n",
            "Ep07 | tr 0.694/0.492 | va 0.687/0.571 | best 0.714\n",
            "Ep08 | tr 0.695/0.554 | va 0.683/0.643 | best 0.714\n",
            "Ep09 | tr 0.697/0.492 | va 0.696/0.500 | best 0.714\n",
            "Ep10 | tr 0.698/0.477 | va 0.677/0.643 | best 0.714\n",
            "Early stop.\n",
            "Image feature shapes: (65, 64) (14, 64) (16, 64)\n",
            "Meta dims: 4\n",
            "Final feature dims: 68\n",
            "[RF] Best VAL threshold: 0.200 (F1=0.667)\n",
            "\n",
            "[RF] TEST ACC: 0.562 | F1: 0.696 | thr=0.200\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[1 7]\n",
            " [0 8]]\n",
            "\n",
            "[LogReg] TEST ACC: 0.625 | F1: 0.625\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[5 3]\n",
            " [3 5]]\n",
            "\n",
            "[LinearSVM] TEST ACC: 0.562 | F1: 0.533\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[5 3]\n",
            " [4 4]]\n",
            "\n",
            "[RF] classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.125     0.222         8\n",
            "           1      0.533     1.000     0.696         8\n",
            "\n",
            "    accuracy                          0.562        16\n",
            "   macro avg      0.767     0.562     0.459        16\n",
            "weighted avg      0.767     0.562     0.459        16\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Combined Evaluation: 3D CNN | RF | Fusion RF =====\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 1) End-to-End 3D CNN Classifier (image only)\n",
        "# --------------------------------------------------------\n",
        "enc.load_state_dict(torch.load(\"/content/enc_best.pt\", map_location=device))  # trained encoder\n",
        "enc.eval()\n",
        "cnn_preds=[]; cnn_true=[]\n",
        "with torch.no_grad():\n",
        "    for x,y,_ in test_dl:\n",
        "        x=x.to(device); y=y.numpy().tolist()\n",
        "        logits = enc(x, feat_only=False)        # directly classifier head\n",
        "        pred = logits.argmax(1).cpu().numpy().tolist()\n",
        "        cnn_preds += pred; cnn_true += y\n",
        "\n",
        "cnn_acc = accuracy_score(cnn_true, cnn_preds)\n",
        "cnn_f1  = f1_score(cnn_true, cnn_preds)\n",
        "cnn_cm  = confusion_matrix(cnn_true, cnn_preds)\n",
        "print(\"\\n[3D CNN] TEST ACC:\", cnn_acc, \"| F1:\", cnn_f1)\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", cnn_cm)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2) RF (image feature only, no metadata)\n",
        "# --------------------------------------------------------\n",
        "rf_img = RandomForestClassifier(n_estimators=400, class_weight=\"balanced\", random_state=SEED)\n",
        "rf_img.fit(Xtr_img, ytr)   # only image features\n",
        "rf_pred = rf_img.predict(Xte_img)\n",
        "rf_acc = accuracy_score(yte, rf_pred)\n",
        "rf_f1  = f1_score(yte, rf_pred)\n",
        "rf_cm  = confusion_matrix(yte, rf_pred)\n",
        "print(\"\\n[RF only-img] TEST ACC:\", rf_acc, \"| F1:\", rf_f1)\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", rf_cm)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3) Hybrid Fusion RF (image + metadata)\n",
        "# --------------------------------------------------------\n",
        "rf_fusion = RandomForestClassifier(n_estimators=800, class_weight=\"balanced\", random_state=SEED)\n",
        "rf_fusion.fit(np.vstack([Xtr_f,Xva_f]), np.hstack([ytr,yva]))\n",
        "fusion_pred = rf_fusion.predict(Xte_f)\n",
        "fusion_acc = accuracy_score(yte, fusion_pred)\n",
        "fusion_f1  = f1_score(yte, fusion_pred)\n",
        "fusion_cm  = confusion_matrix(yte, fusion_pred)\n",
        "print(\"\\n[Fusion RF] TEST ACC:\", fusion_acc, \"| F1:\", fusion_f1)\n",
        "print(\"CM [[TN FP],[FN TP]]:\\n\", fusion_cm)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Summary\n",
        "# --------------------------------------------------------\n",
        "print(\"\\n==== SUMMARY ====\")\n",
        "print(f\"3D CNN         → ACC {cnn_acc:.3f} | F1 {cnn_f1:.3f}\")\n",
        "print(f\"RF (img-only)  → ACC {rf_acc:.3f} | F1 {rf_f1:.3f}\")\n",
        "print(f\"Fusion RF(img+meta) → ACC {fusion_acc:.3f} | F1 {fusion_f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI2Q4MinRif-",
        "outputId": "77f1a73e-1256-4e44-ef59-44fe50972657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3D CNN] TEST ACC: 0.625 | F1: 0.7\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[3 5]\n",
            " [1 7]]\n",
            "\n",
            "[RF only-img] TEST ACC: 0.6875 | F1: 0.7058823529411765\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[5 3]\n",
            " [2 6]]\n",
            "\n",
            "[Fusion RF] TEST ACC: 0.8125 | F1: 0.8235294117647058\n",
            "CM [[TN FP],[FN TP]]:\n",
            " [[6 2]\n",
            " [1 7]]\n",
            "\n",
            "==== SUMMARY ====\n",
            "3D CNN         → ACC 0.625 | F1 0.700\n",
            "RF (img-only)  → ACC 0.688 | F1 0.706\n",
            "Fusion RF(img+meta) → ACC 0.812 | F1 0.824\n"
          ]
        }
      ]
    }
  ]
}